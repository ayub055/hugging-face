{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from tqdm import tqdm  # instead of tqdm.auto\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "loading configuration file config.json from cache at /Users/ayyoob/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased-finetuned-sst-2-english/snapshots/714eb0fa89d2f80546fda750413ed43d93601a13/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.54.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /Users/ayyoob/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased-finetuned-sst-2-english/snapshots/714eb0fa89d2f80546fda750413ed43d93601a13/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.54.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /Users/ayyoob/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased-finetuned-sst-2-english/snapshots/714eb0fa89d2f80546fda750413ed43d93601a13/model.safetensors\n",
      "Since the `torch_dtype` attribute can't be found in model's config object, will use torch_dtype={torch_dtype} as derived from model's weights\n",
      "Instantiating DistilBertForSequenceClassification model under default dtype torch.float32.\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at distilbert/distilbert-base-uncased-finetuned-sst-2-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n",
      "loading configuration file config.json from cache at /Users/ayyoob/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased-finetuned-sst-2-english/snapshots/714eb0fa89d2f80546fda750413ed43d93601a13/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.54.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /Users/ayyoob/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased-finetuned-sst-2-english/snapshots/714eb0fa89d2f80546fda750413ed43d93601a13/vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /Users/ayyoob/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased-finetuned-sst-2-english/snapshots/714eb0fa89d2f80546fda750413ed43d93601a13/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at /Users/ayyoob/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased-finetuned-sst-2-english/snapshots/714eb0fa89d2f80546fda750413ed43d93601a13/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.54.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /Users/ayyoob/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased-finetuned-sst-2-english/snapshots/714eb0fa89d2f80546fda750413ed43d93601a13/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.54.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"text-classification\")\n",
    "text = \"\"\"Dear Amazon, last week I ordered an Optimus Prime action figure \\\n",
    "from your online store in Germany. Unfortunately, when I opened the package, \\\n",
    "I discovered to my horror that I had been sent an action figure of Megatron \\\n",
    "instead! As a lifelong enemy of the Decepticons, I hope you can understand my \\\n",
    "dilemma. To resolve the issue, I demand an exchange of Megatron for the \\\n",
    "Optimus Prime figure I ordered. Enclosed are copies of my records concerning \\\n",
    "this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : audio-classification\n",
      "Model : automatic-speech-recognition\n",
      "Model : text-to-audio\n",
      "Model : feature-extraction\n",
      "Model : text-classification\n",
      "Model : token-classification\n",
      "Model : question-answering\n",
      "Model : table-question-answering\n",
      "Model : visual-question-answering\n",
      "Model : document-question-answering\n",
      "Model : fill-mask\n",
      "Model : summarization\n",
      "Model : translation\n",
      "Model : text2text-generation\n",
      "Model : text-generation\n",
      "Model : zero-shot-classification\n",
      "Model : zero-shot-image-classification\n",
      "Model : zero-shot-audio-classification\n",
      "Model : image-classification\n",
      "Model : image-feature-extraction\n",
      "Model : image-segmentation\n",
      "Model : image-to-text\n",
      "Model : image-text-to-text\n",
      "Model : object-detection\n",
      "Model : zero-shot-object-detection\n",
      "Model : depth-estimation\n",
      "Model : video-classification\n",
      "Model : mask-generation\n",
      "Model : image-to-image\n"
     ]
    }
   ],
   "source": [
    "# from transformers import SUPPORTED_TASKS # Gives error\n",
    "from transformers.pipelines import SUPPORTED_TASKS\n",
    "# print(SUPPORTED_TASKS.keys())\n",
    "\n",
    "for key, val in SUPPORTED_TASKS.items():\n",
    "    print(f\"Model : {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.901545524597168}]\n"
     ]
    }
   ],
   "source": [
    "result = classifier(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "loading configuration file config.json from cache at /Users/ayyoob/.cache/huggingface/hub/models--dbmdz--bert-large-cased-finetuned-conll03-english/snapshots/4c534963167c08d4b8ff1f88733cf2930f86add0/config.json\n",
      "Model config BertConfig {\n",
      "  \"_num_labels\": 9,\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-MISC\",\n",
      "    \"2\": \"I-MISC\",\n",
      "    \"3\": \"B-PER\",\n",
      "    \"4\": \"I-PER\",\n",
      "    \"5\": \"B-ORG\",\n",
      "    \"6\": \"I-ORG\",\n",
      "    \"7\": \"B-LOC\",\n",
      "    \"8\": \"I-LOC\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"B-LOC\": 7,\n",
      "    \"B-MISC\": 1,\n",
      "    \"B-ORG\": 5,\n",
      "    \"B-PER\": 3,\n",
      "    \"I-LOC\": 8,\n",
      "    \"I-MISC\": 2,\n",
      "    \"I-ORG\": 6,\n",
      "    \"I-PER\": 4,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.54.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /Users/ayyoob/.cache/huggingface/hub/models--dbmdz--bert-large-cased-finetuned-conll03-english/snapshots/4c534963167c08d4b8ff1f88733cf2930f86add0/config.json\n",
      "Model config BertConfig {\n",
      "  \"_num_labels\": 9,\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-MISC\",\n",
      "    \"2\": \"I-MISC\",\n",
      "    \"3\": \"B-PER\",\n",
      "    \"4\": \"I-PER\",\n",
      "    \"5\": \"B-ORG\",\n",
      "    \"6\": \"I-ORG\",\n",
      "    \"7\": \"B-LOC\",\n",
      "    \"8\": \"I-LOC\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"B-LOC\": 7,\n",
      "    \"B-MISC\": 1,\n",
      "    \"B-ORG\": 5,\n",
      "    \"B-PER\": 3,\n",
      "    \"I-LOC\": 8,\n",
      "    \"I-MISC\": 2,\n",
      "    \"I-ORG\": 6,\n",
      "    \"I-PER\": 4,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.54.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /Users/ayyoob/.cache/huggingface/hub/models--dbmdz--bert-large-cased-finetuned-conll03-english/snapshots/4c534963167c08d4b8ff1f88733cf2930f86add0/model.safetensors\n",
      "Since the `torch_dtype` attribute can't be found in model's config object, will use torch_dtype={torch_dtype} as derived from model's weights\n",
      "Instantiating BertForTokenClassification model under default dtype torch.float32.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertForTokenClassification were initialized from the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForTokenClassification for predictions without further training.\n",
      "loading configuration file config.json from cache at /Users/ayyoob/.cache/huggingface/hub/models--dbmdz--bert-large-cased-finetuned-conll03-english/snapshots/4c534963167c08d4b8ff1f88733cf2930f86add0/config.json\n",
      "Model config BertConfig {\n",
      "  \"_num_labels\": 9,\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-MISC\",\n",
      "    \"2\": \"I-MISC\",\n",
      "    \"3\": \"B-PER\",\n",
      "    \"4\": \"I-PER\",\n",
      "    \"5\": \"B-ORG\",\n",
      "    \"6\": \"I-ORG\",\n",
      "    \"7\": \"B-LOC\",\n",
      "    \"8\": \"I-LOC\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"B-LOC\": 7,\n",
      "    \"B-MISC\": 1,\n",
      "    \"B-ORG\": 5,\n",
      "    \"B-PER\": 3,\n",
      "    \"I-LOC\": 8,\n",
      "    \"I-MISC\": 2,\n",
      "    \"I-ORG\": 6,\n",
      "    \"I-PER\": 4,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.54.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /Users/ayyoob/.cache/huggingface/hub/models--dbmdz--bert-large-cased-finetuned-conll03-english/snapshots/4c534963167c08d4b8ff1f88733cf2930f86add0/vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /Users/ayyoob/.cache/huggingface/hub/models--dbmdz--bert-large-cased-finetuned-conll03-english/snapshots/4c534963167c08d4b8ff1f88733cf2930f86add0/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at /Users/ayyoob/.cache/huggingface/hub/models--dbmdz--bert-large-cased-finetuned-conll03-english/snapshots/4c534963167c08d4b8ff1f88733cf2930f86add0/config.json\n",
      "Model config BertConfig {\n",
      "  \"_num_labels\": 9,\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-MISC\",\n",
      "    \"2\": \"I-MISC\",\n",
      "    \"3\": \"B-PER\",\n",
      "    \"4\": \"I-PER\",\n",
      "    \"5\": \"B-ORG\",\n",
      "    \"6\": \"I-ORG\",\n",
      "    \"7\": \"B-LOC\",\n",
      "    \"8\": \"I-LOC\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"B-LOC\": 7,\n",
      "    \"B-MISC\": 1,\n",
      "    \"B-ORG\": 5,\n",
      "    \"B-PER\": 3,\n",
      "    \"I-LOC\": 8,\n",
      "    \"I-MISC\": 2,\n",
      "    \"I-ORG\": 6,\n",
      "    \"I-PER\": 4,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.54.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /Users/ayyoob/.cache/huggingface/hub/models--dbmdz--bert-large-cased-finetuned-conll03-english/snapshots/4c534963167c08d4b8ff1f88733cf2930f86add0/config.json\n",
      "Model config BertConfig {\n",
      "  \"_num_labels\": 9,\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-MISC\",\n",
      "    \"2\": \"I-MISC\",\n",
      "    \"3\": \"B-PER\",\n",
      "    \"4\": \"I-PER\",\n",
      "    \"5\": \"B-ORG\",\n",
      "    \"6\": \"I-ORG\",\n",
      "    \"7\": \"B-LOC\",\n",
      "    \"8\": \"I-LOC\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"B-LOC\": 7,\n",
      "    \"B-MISC\": 1,\n",
      "    \"B-ORG\": 5,\n",
      "    \"B-PER\": 3,\n",
      "    \"I-LOC\": 8,\n",
      "    \"I-MISC\": 2,\n",
      "    \"I-ORG\": 6,\n",
      "    \"I-PER\": 4,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.54.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayyoob/anaconda3/envs/hugging_face/lib/python3.10/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_group</th>\n",
       "      <th>score</th>\n",
       "      <th>word</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.879009</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.990859</td>\n",
       "      <td>Optimus Prime</td>\n",
       "      <td>36</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LOC</td>\n",
       "      <td>0.999755</td>\n",
       "      <td>Germany</td>\n",
       "      <td>90</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.556567</td>\n",
       "      <td>Mega</td>\n",
       "      <td>208</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.590256</td>\n",
       "      <td>##tron</td>\n",
       "      <td>212</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.669692</td>\n",
       "      <td>Decept</td>\n",
       "      <td>253</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.498349</td>\n",
       "      <td>##icons</td>\n",
       "      <td>259</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.775361</td>\n",
       "      <td>Megatron</td>\n",
       "      <td>350</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.987854</td>\n",
       "      <td>Optimus Prime</td>\n",
       "      <td>367</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.812096</td>\n",
       "      <td>Bumblebee</td>\n",
       "      <td>502</td>\n",
       "      <td>511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entity_group     score           word  start  end\n",
       "0          ORG  0.879009         Amazon      5   11\n",
       "1         MISC  0.990859  Optimus Prime     36   49\n",
       "2          LOC  0.999755        Germany     90   97\n",
       "3         MISC  0.556567           Mega    208  212\n",
       "4          PER  0.590256         ##tron    212  216\n",
       "5          ORG  0.669692         Decept    253  259\n",
       "6         MISC  0.498349        ##icons    259  264\n",
       "7         MISC  0.775361       Megatron    350  358\n",
       "8         MISC  0.987854  Optimus Prime    367  380\n",
       "9          PER  0.812096      Bumblebee    502  511"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = ner_tagger(text)\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayyoob/anaconda3/envs/hugging_face/lib/python3.10/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_group</th>\n",
       "      <th>score</th>\n",
       "      <th>word</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.9707</td>\n",
       "      <td>Suraj Pharmacy</td>\n",
       "      <td>53</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entity_group   score            word  start  end\n",
       "0          ORG  0.9707  Suraj Pharmacy     53   67"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = ner_tagger('Customer 23232 did NEFT transaction of amount 500 to Suraj Pharmacy', aggregation_strategy=\"simple\")\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(result)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "loading configuration file config.json from cache at /Users/ayyoob/.cache/huggingface/hub/models--distilbert--distilbert-base-cased-distilled-squad/snapshots/564e9b582944a57a3e586bbb98fd6f0a4118db7f/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": true,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.54.1\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /Users/ayyoob/.cache/huggingface/hub/models--distilbert--distilbert-base-cased-distilled-squad/snapshots/564e9b582944a57a3e586bbb98fd6f0a4118db7f/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": true,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.54.1\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /Users/ayyoob/.cache/huggingface/hub/models--distilbert--distilbert-base-cased-distilled-squad/snapshots/564e9b582944a57a3e586bbb98fd6f0a4118db7f/model.safetensors\n",
      "Since the `torch_dtype` attribute can't be found in model's config object, will use torch_dtype={torch_dtype} as derived from model's weights\n",
      "Instantiating DistilBertForQuestionAnswering model under default dtype torch.float32.\n",
      "All model checkpoint weights were used when initializing DistilBertForQuestionAnswering.\n",
      "\n",
      "All the weights of DistilBertForQuestionAnswering were initialized from the model checkpoint at distilbert/distilbert-base-cased-distilled-squad.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForQuestionAnswering for predictions without further training.\n",
      "loading configuration file config.json from cache at /Users/ayyoob/.cache/huggingface/hub/models--distilbert--distilbert-base-cased-distilled-squad/snapshots/564e9b582944a57a3e586bbb98fd6f0a4118db7f/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": true,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.54.1\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /Users/ayyoob/.cache/huggingface/hub/models--distilbert--distilbert-base-cased-distilled-squad/snapshots/564e9b582944a57a3e586bbb98fd6f0a4118db7f/vocab.txt\n",
      "loading file tokenizer.json from cache at /Users/ayyoob/.cache/huggingface/hub/models--distilbert--distilbert-base-cased-distilled-squad/snapshots/564e9b582944a57a3e586bbb98fd6f0a4118db7f/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /Users/ayyoob/.cache/huggingface/hub/models--distilbert--distilbert-base-cased-distilled-squad/snapshots/564e9b582944a57a3e586bbb98fd6f0a4118db7f/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at /Users/ayyoob/.cache/huggingface/hub/models--distilbert--distilbert-base-cased-distilled-squad/snapshots/564e9b582944a57a3e586bbb98fd6f0a4118db7f/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": true,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.54.1\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /Users/ayyoob/.cache/huggingface/hub/models--distilbert--distilbert-base-cased-distilled-squad/snapshots/564e9b582944a57a3e586bbb98fd6f0a4118db7f/config.json\n",
      "loading configuration file config.json from cache at /Users/ayyoob/.cache/huggingface/hub/models--distilbert--distilbert-base-cased-distilled-squad/snapshots/564e9b582944a57a3e586bbb98fd6f0a4118db7f/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": true,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"auto\",\n",
      "  \"transformers_version\": \"4.54.1\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /Users/ayyoob/.cache/huggingface/hub/models--distilbert--distilbert-base-cased-distilled-squad/snapshots/564e9b582944a57a3e586bbb98fd6f0a4118db7f/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": true,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"auto\",\n",
      "  \"transformers_version\": \"4.54.1\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /Users/ayyoob/.cache/huggingface/hub/models--distilbert--distilbert-base-cased-distilled-squad/snapshots/564e9b582944a57a3e586bbb98fd6f0a4118db7f/vocab.txt\n",
      "loading file tokenizer.json from cache at /Users/ayyoob/.cache/huggingface/hub/models--distilbert--distilbert-base-cased-distilled-squad/snapshots/564e9b582944a57a3e586bbb98fd6f0a4118db7f/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /Users/ayyoob/.cache/huggingface/hub/models--distilbert--distilbert-base-cased-distilled-squad/snapshots/564e9b582944a57a3e586bbb98fd6f0a4118db7f/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at /Users/ayyoob/.cache/huggingface/hub/models--distilbert--distilbert-base-cased-distilled-squad/snapshots/564e9b582944a57a3e586bbb98fd6f0a4118db7f/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": true,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.54.1\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.759297</td>\n",
       "      <td>33</td>\n",
       "      <td>63</td>\n",
       "      <td>an Optimus Prime action figure</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      score  start  end                          answer\n",
       "0  0.759297     33   63  an Optimus Prime action figure"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader = pipeline('question-answering')\n",
    "question = \"What did Bumblebee order from Amazon?\"\n",
    "result = reader(question=question, context=text)\n",
    "pd.DataFrame([result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "summariser = pipeline(\"summarization\")\n",
    "output = summariser(text, max_length=50, min_length=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bumblebee ordered an Optimus Prime action figure from your online store in Germany . Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead .\n"
     ]
    }
   ],
   "source": [
    "# print(output) # Output is a list of dictionaries (only one with key : summary_text)\n",
    "print(output[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging_face",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
